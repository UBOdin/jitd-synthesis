package org.apache.spark.sql.execution.datasources;
public  class InsertIntoHadoopFsRelationCommand$ extends scala.runtime.AbstractFunction12<org.apache.hadoop.fs.Path, scala.collection.immutable.Map<java.lang.String, java.lang.String>, java.lang.Object, scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Attribute>, scala.Option<org.apache.spark.sql.catalyst.catalog.BucketSpec>, org.apache.spark.sql.execution.datasources.FileFormat, scala.collection.immutable.Map<java.lang.String, java.lang.String>, org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, org.apache.spark.sql.SaveMode, scala.Option<org.apache.spark.sql.catalyst.catalog.CatalogTable>, scala.Option<org.apache.spark.sql.execution.datasources.FileIndex>, scala.collection.Seq<java.lang.String>, org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand> implements scala.Serializable {
  /**
   * Static reference to the singleton instance of this Scala object.
   */
  public static final InsertIntoHadoopFsRelationCommand$ MODULE$ = null;
  public   InsertIntoHadoopFsRelationCommand$ ()  { throw new RuntimeException(); }
}
