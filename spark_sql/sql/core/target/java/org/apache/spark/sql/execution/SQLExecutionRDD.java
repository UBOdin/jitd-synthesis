package org.apache.spark.sql.execution;
/**
 * It is just a wrapper over <code>sqlRDD</code>, which sets and makes effective all the configs from the
 * captured <code>SQLConf</code>.
 * Please notice that this means we may miss configurations set after the creation of this RDD and
 * before its execution.
 * <p>
 * param:  sqlRDD the <code>RDD</code> generated by the SQL plan
 * param:  conf the <code>SQLConf</code> to apply to the execution of the SQL plan
 */
public  class SQLExecutionRDD extends org.apache.spark.rdd.RDD<org.apache.spark.sql.catalyst.InternalRow> {
  // not preceding
  public   SQLExecutionRDD (org.apache.spark.rdd.RDD<org.apache.spark.sql.catalyst.InternalRow> sqlRDD, org.apache.spark.sql.internal.SQLConf conf)  { throw new RuntimeException(); }
  public  scala.collection.Iterator<org.apache.spark.sql.catalyst.InternalRow> compute (org.apache.spark.Partition split, org.apache.spark.TaskContext context)  { throw new RuntimeException(); }
  public  org.apache.spark.Partition[] getPartitions ()  { throw new RuntimeException(); }
  public  scala.Option<org.apache.spark.Partitioner> partitioner ()  { throw new RuntimeException(); }
  public  org.apache.spark.rdd.RDD<org.apache.spark.sql.catalyst.InternalRow> sqlRDD ()  { throw new RuntimeException(); }
}
