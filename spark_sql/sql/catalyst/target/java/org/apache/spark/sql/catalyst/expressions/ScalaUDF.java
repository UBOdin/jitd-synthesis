package org.apache.spark.sql.catalyst.expressions;
/**
 * User-defined function.
 * param:  function  The user defined scala function to run.
 *                  Note that if you use primitive parameters, you are not able to check if it is
 *                  null or not, and the UDF will return null for you if the primitive input is
 *                  null. Use boxed type or {@link Option} if you wanna do the null-handling yourself.
 * param:  dataType  Return type of function.
 * param:  children  The input expressions of this UDF.
 * param:  inputEncoders ExpressionEncoder for each input parameters. For a input parameter which
 *                      serialized as struct will use encoder instead of CatalystTypeConverters to
 *                      convert internal value to Scala value.
 * param:  outputEncoder ExpressionEncoder for the return type of function. It's only defined when
 *                      this is a typed Scala UDF.
 * param:  udfName  The user-specified name of this UDF.
 * param:  nullable  True if the UDF can return null value.
 * param:  udfDeterministic  True if the UDF is deterministic. Deterministic UDF returns same result
 *                          each time it is invoked with a particular input.
 */
public  class ScalaUDF extends org.apache.spark.sql.catalyst.expressions.Expression implements org.apache.spark.sql.catalyst.expressions.NonSQLExpression, org.apache.spark.sql.catalyst.expressions.UserDefinedExpression, scala.Product, scala.Serializable {
  static public abstract  R apply (T1 v1, T2 v2, T3 v3, T4 v4, T5 v5, T6 v6, T7 v7, T8 v8)  ;
  public  java.lang.Object function ()  { throw new RuntimeException(); }
  public  org.apache.spark.sql.types.DataType dataType ()  { throw new RuntimeException(); }
  public  scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Expression> children ()  { throw new RuntimeException(); }
  public  scala.collection.Seq<scala.Option<org.apache.spark.sql.catalyst.encoders.ExpressionEncoder<?>>> inputEncoders ()  { throw new RuntimeException(); }
  public  scala.Option<org.apache.spark.sql.catalyst.encoders.ExpressionEncoder<?>> outputEncoder ()  { throw new RuntimeException(); }
  public  scala.Option<java.lang.String> udfName ()  { throw new RuntimeException(); }
  public  boolean nullable ()  { throw new RuntimeException(); }
  public  boolean udfDeterministic ()  { throw new RuntimeException(); }
  // not preceding
  public   ScalaUDF (java.lang.Object function, org.apache.spark.sql.types.DataType dataType, scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Expression> children, scala.collection.Seq<scala.Option<org.apache.spark.sql.catalyst.encoders.ExpressionEncoder<?>>> inputEncoders, scala.Option<org.apache.spark.sql.catalyst.encoders.ExpressionEncoder<?>> outputEncoder, scala.Option<java.lang.String> udfName, boolean nullable, boolean udfDeterministic)  { throw new RuntimeException(); }
  // not preceding
  public  boolean deterministic ()  { throw new RuntimeException(); }
  public  java.lang.String toString ()  { throw new RuntimeException(); }
  public  java.lang.String name ()  { throw new RuntimeException(); }
  // not preceding
  public  org.apache.spark.sql.catalyst.expressions.Expression canonicalized ()  { throw new RuntimeException(); }
  // not preceding
  public  scala.collection.Seq<java.lang.Object> inputPrimitives ()  { throw new RuntimeException(); }
  /**
   * The expected input types of this UDF, used to perform type coercion. If we do
   * not want to perform coercion, simply use "Nil". Note that it would've been
   * better to use Option of Seq[DataType] so we can use "None" as the case for no
   * type coercion. However, that would require more refactoring of the codebase.
   * @return (undocumented)
   */
  public  scala.collection.Seq<org.apache.spark.sql.types.AbstractDataType> inputTypes ()  { throw new RuntimeException(); }
  /** This method has been generated by this script
   * <p>
   (1 to 22).map { x =&gt;
   val anys = (1 to x).map(x =&gt; "Any").reduce(_ + ", " + _)
   val childs = (0 to x - 1).map(x =&gt; s"val child$x = children($x)").reduce(_ + "\n  " + _)
   val converters = (0 to x - 1).map(x =&gt; s"lazy val converter$x = createToScalaConverter($x, child$x.dataType)").reduce(_ + "\n  " + _)
   val evals = (0 to x - 1).map(x =&gt; s"converter$x(child$x.eval(input))").reduce(_ + ",\n      " + _)
   * <p>
   s"""case $x =&gt;
   val func = function.asInstanceOf[($anys) =&gt; Any]
   $childs
   $converters
   (input: InternalRow) =&gt; {
   func(
   $evals)
   }
   """
   }.foreach(println)
   * <p>
   * @param ctx (undocumented)
   * @param ev (undocumented)
   * @return (undocumented)
   */
  public  org.apache.spark.sql.catalyst.expressions.codegen.ExprCode doGenCode (org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext ctx, org.apache.spark.sql.catalyst.expressions.codegen.ExprCode ev)  { throw new RuntimeException(); }
  // not preceding
  public  java.lang.String funcCls ()  { throw new RuntimeException(); }
  // not preceding
  public  java.lang.String inputTypesString ()  { throw new RuntimeException(); }
  // not preceding
  public  java.lang.String outputType ()  { throw new RuntimeException(); }
  public  Object eval (org.apache.spark.sql.catalyst.InternalRow input)  { throw new RuntimeException(); }
}
